{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Embeddings Extraction Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-26 12:24:27.190098: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-26 12:24:29.502443: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "import umap.umap_ as umap\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.patches as mpatches\n",
    "from tensorboard.plugins import projector\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loading and temp+test images extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/home/ashgatsy/DeepPCB-master/PCBData\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 89.60it/s]\n"
     ]
    }
   ],
   "source": [
    "normal = []\n",
    "defect = []\n",
    "defectlog = []\n",
    "path_2 = [os.path.join(PATH,dir) for dir in os.listdir(PATH) if '.' not in dir]\n",
    "for p in tqdm(path_2,total=len(path_2)):\n",
    "    path_3 = os.path.join(p,sorted(os.listdir(p))[0])\n",
    "    normal +=[os.path.join(path_3,dir)for dir in os.listdir(path_3) if 'temp' in dir]\n",
    "    defect +=[os.path.join(path_3,dir)for dir in os.listdir(path_3) if 'test' in dir]\n",
    "    path_4 = os.path.join(p,sorted(os.listdir(p))[1])\n",
    "    defectlog +=[os.path.join(path_4,dir)for dir in os.listdir(path_4)]\n",
    "normal.sort()\n",
    "defect.sort()\n",
    "defectlog.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1501 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1501/1501 [00:04<00:00, 370.64it/s]\n",
      "100%|██████████| 1500/1500 [00:02<00:00, 506.49it/s]\n"
     ]
    }
   ],
   "source": [
    "img0 = []\n",
    "img1 = []\n",
    "for img_path in tqdm(normal,total=len(normal)):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.resize(img,(128,128))\n",
    "    img0.append(img)\n",
    "for img_path in tqdm(defect,total=len(defect)):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.resize(img,(128,128))\n",
    "    img1.append(img)\n",
    "img0 = np.array(img0)\n",
    "img1 = np.array(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_to_rgb(bnw_images):\n",
    "    # function that converts numpy array of bnw images to numpy array of rgb images\n",
    "    num_images, height, width = bnw_images.shape\n",
    "    rgb_images = np.zeros((num_images, height, width, 3), dtype=np.uint8)\n",
    "    # Set all three channels to the same intensity (grayscale value)\n",
    "    for i in range(num_images):\n",
    "        rgb_images[i, :, :, 0] = bnw_images[i]  # Red channel\n",
    "        rgb_images[i, :, :, 1] = bnw_images[i]  # Green channel\n",
    "        rgb_images[i, :, :, 2] = bnw_images[i]  # Blue channel\n",
    "\n",
    "    rgb_images = rgb_images.transpose(0, 3, 1, 2)\n",
    "    return rgb_images\n",
    "\n",
    "rgb_images0 = conv_to_rgb(img0)\n",
    "rgb_images1 = conv_to_rgb(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images, height, width = img0.shape\n",
    "rgb_images = np.zeros((num_images, height, width, 3), dtype=np.uint8)\n",
    "\n",
    "# Set all three channels to the same intensity (grayscale value)\n",
    "for i in range(num_images):\n",
    "    rgb_images[i, :, :, 0] = img0[i]  # Red channel\n",
    "    rgb_images[i, :, :, 1] = img0[i]  # Green channel\n",
    "    rgb_images[i, :, :, 2] = img0[i]  # Blue channel\n",
    "\n",
    "rgb_images = rgb_images.transpose(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Pre-trained Model and get the dataset transformed according to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ashgatsy/.cache/torch/hub/facebookresearch_WSL-Images_main\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x8d_wsl')\n",
    "\n",
    "def slice_model(original_model, from_layer=None, to_layer=None):\n",
    "    return nn.Sequential(*list(original_model.children())[from_layer:to_layer])\n",
    "\n",
    "model_conv_features = slice_model(model, to_layer=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define required transformations according the model imported\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    # transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transforms(rgb_images,transform):\n",
    "    # function to apply required transformation on the dataset images\n",
    "    transformed_images0 = []\n",
    "    rgb_images = rgb_images.transpose(0, 2, 3, 1)\n",
    "    for img_np in rgb_images:\n",
    "        # Convert numpy array to PIL Image\n",
    "        # print(img_np.shape)\n",
    "        img_pil = Image.fromarray(img_np)\n",
    "\n",
    "        # Apply the transformation\n",
    "        img_transformed = transform(img_pil)\n",
    "\n",
    "        # Convert the transformed image back to numpy array\n",
    "        img_transformed_np = np.array(img_transformed)\n",
    "\n",
    "        # Append to the list\n",
    "        transformed_images0.append(img_transformed_np)\n",
    "    return transformed_images0\n",
    "\n",
    "transformed_images0 = apply_transforms(rgb_images0,preprocess)\n",
    "transformed_images1 = apply_transforms(rgb_images1,preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, targets=None, transform=None):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        if self.targets is not None:\n",
    "            return sample, self.targets[idx]\n",
    "        else:\n",
    "            return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset0 = MyDataset(data=transformed_images0)\n",
    "my_dataset1 = MyDataset(data=transformed_images1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpus 4\n"
     ]
    }
   ],
   "source": [
    "cpu_count = multiprocessing.cpu_count()\n",
    "print('cpus', cpu_count)\n",
    "batch_size = 8\n",
    "my_dataloader0 = DataLoader(dataset=my_dataset0, batch_size=batch_size, shuffle=True, num_workers=cpu_count)\n",
    "my_dataloader1 = DataLoader(dataset=my_dataset1, batch_size=batch_size, shuffle=True, num_workers=cpu_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(dataloader,model_conv_features):\n",
    "    features_list=[]\n",
    "    # this function returns the populated features list\n",
    "    model_conv_features\n",
    "    for batch in tqdm(dataloader):\n",
    "        # print(batch.shape)\n",
    "        image_batch = batch\n",
    "        # image_batch = torch.stack(image_batch) \n",
    "\n",
    "        with torch.no_grad():\n",
    "            features_batch = model_conv_features(image_batch).flatten(start_dim=1)\n",
    "        features_list.append(features_batch)\n",
    "\n",
    "    return features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [24:44<00:00,  7.90s/it]\n",
      "100%|██████████| 188/188 [24:31<00:00,  7.83s/it]\n"
     ]
    }
   ],
   "source": [
    "features_list0 = get_features(my_dataloader0,model_conv_features)\n",
    "features_list1 = get_features(my_dataloader1,model_conv_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_list0[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the embeddings on qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "qdrant = QdrantClient(url='http://132.186.158.40:6333/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_points(collection_name,client,num_vectors):\n",
    "    res = client.scroll(\n",
    "    collection_name=collection_name,\n",
    "    scroll_filter=None,\n",
    "    limit=num_vectors,\n",
    "    with_payload=True,\n",
    "    with_vectors=True,\n",
    "    )\n",
    "    return res\n",
    "\n",
    "ret_points0 = retrieve_points(\"check1\",qdrant,2)\n",
    "ret_points1 = retrieve_points(\"check2\",qdrant,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(points):\n",
    "    ret_embeddings = []\n",
    "    ret_ids = [] #no need to keep anything here since id is just the index of the retrieved vector. (int(ret_points0[0][0].payload['text'].split()[1]))\n",
    "    for record in points[0]:\n",
    "        ret_embeddings.append(record.vector)\n",
    "\n",
    "    return ret_embeddings\n",
    "\n",
    "ret_embeddings0 = extract_embeddings(ret_points0)\n",
    "ret_embeddings1 = extract_embeddings(ret_points1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def concatenate_embeddings(features_list):\n",
    "#   # Reshape each tensor to remove the batch dimension\n",
    "#   embeddings = [tf.reshape(tensor, [-1, 2048]) for tensor in features_list]\n",
    "\n",
    "#   # Concatenate the reshaped tensors\n",
    "#   concatenated_embeddings = tf.concat(embeddings, axis=0)\n",
    "\n",
    "#   return concatenated_embeddings.numpy().tolist()\n",
    "\n",
    "# individual_embeddings0 = concatenate_embeddings(features_list0)\n",
    "# individual_embeddings1 = concatenate_embeddings(features_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings inserted successfully!\n",
      "Embeddings inserted successfully!\n"
     ]
    }
   ],
   "source": [
    "# def qdarnt_upload(collection_name,csize,vectors):\n",
    "#     qdrant.create_collection(\n",
    "#         collection_name=collection_name,\n",
    "#         vectors_config=VectorParams(size=2048,distance=Distance.COSINE),\n",
    "#     )\n",
    "#     for i, embedding in enumerate(vectors):\n",
    "#         qdrant.upsert(collection_name=collection_name, points=[{\n",
    "#             'id': i,  # Unique ID for each data point\n",
    "#             'vector': embedding,  # Convert numpy array to list\n",
    "#             'payload': {'text': f'Embedding {i}'}  # Optional payload (metadata)\n",
    "#         }])\n",
    "#     print(\"Embeddings inserted successfully!\")\n",
    "\n",
    "# qdarnt_upload(\"check1\",1501,individual_embeddings0)\n",
    "# qdarnt_upload(\"check2\",1500,individual_embeddings1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Means based clustering of all the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_embeddings(embeddings, num_clusters):\n",
    "    # Flatten the list of tensors into a single 2D array\n",
    "    flattened_embeddings = np.array(embeddings)\n",
    "\n",
    "    # Perform K-Means clustering\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=0)  # Set random_state for reproducibility\n",
    "    kmeans.fit(flattened_embeddings)\n",
    "\n",
    "    # Get cluster labels for each data point\n",
    "    cluster_labels = kmeans.labels_\n",
    "\n",
    "    return cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1501"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(individual_embeddings0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels0 = cluster_embeddings(individual_embeddings0,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1501"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cluster_labels0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a UMAP using the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashgatsy/miniconda3/envs/env1/lib/python3.11/site-packages/umap/umap_.py:1945: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(f\"n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.\")\n"
     ]
    }
   ],
   "source": [
    "def reduce_dim(embeddings,num_dim,seed):\n",
    "    reducer = umap.UMAP(n_components=num_dim,random_state=seed)\n",
    "    umap_embeddings = reducer.fit_transform(embeddings)\n",
    "    return umap_embeddings\n",
    "\n",
    "umap_embeddings0 = reduce_dim(individual_embeddings0,3,42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir='/home/ashgatsy/test/logs/pcb-example1/'\n",
    "\n",
    "def tensorboard_umap(umap_embeddings,cluster_labels):\n",
    "    embedding_var = tf.Variable(umap_embeddings,name='embedding')\n",
    "    checkpoint = tf.train.Checkpoint(embedding=embedding_var)\n",
    "    checkpoint.save(os.path.join(log_dir,'ts_embedding.ckpt'))\n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding = config.embeddings.add()\n",
    "    embedding.tensor_name = 'embedding/.ATTRIBUTES/VARIABLE_VALUE'  # Assuming saved tensor\n",
    "\n",
    "    # Create metadata.tsv (optional): Each line: index<tab>cluster_label\n",
    "    with open('/home/ashgatsy/test/logs/pcb-example1/metadata.tsv', 'w') as f:\n",
    "        column_names = [\"Feature1\", \"Feature2\", \"Cluster_Label\"]\n",
    "        f.write('\\t'.join(column_names) + '\\n')\n",
    "        for i, label in enumerate(cluster_labels):\n",
    "            f.write(f\"{i}\\t{label}\\n\")\n",
    "\n",
    "    embedding.metadata_path = 'metadata.tsv'\n",
    "\n",
    "    projector.visualize_embeddings(log_dir, config)\n",
    "\n",
    "tensorboard_umap(individual_embeddings0,cluster_labels0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(\n",
    "#     umap_embeddings0[:, 0],\n",
    "#     umap_embeddings0[:, 1],\n",
    "#     c=[sns.color_palette()[label] for label in cluster_labels0])\n",
    "# plt.gca().set_aspect('equal', 'datalim')\n",
    "# plt.title('UMAP projection with 6 clusters', fontsize=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import tensorflow as tf\n",
    "# import tensorflow_datasets as tfds\n",
    "# from tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_var = tf.Variable(individual_embeddings0, name='embedding')\n",
    "# embedding_var1 = tf.Variable(individual_embeddings1, name='embedding1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_dir='/home/ashgatsy/test/logs/pcb-example/'\n",
    "# checkpoint0 = tf.train.Checkpoint(embedding=embedding_var)\n",
    "# checkpoint1 = tf.train.Checkpoint(embedding=embedding_var1)\n",
    "# checkpoint0.save(os.path.join(log_dir, 'embedding.ckpt'))\n",
    "# checkpoint1.save(os.path.join(log_dir, 'embedding.ckpt1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set up projector config\n",
    "# config = projector.ProjectorConfig()\n",
    "# embedding = config.embeddings.add()\n",
    "# # The name of the tensor will be suffixed by `/.ATTRIBUTES/VARIABLE_VALUE`.\n",
    "# embedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
    "# embedding.metadata_path = 'metadata.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# projector.visualize_embeddings(log_dir, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-24 21:56:45.191936: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-24 21:56:46.750599: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.16.2 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir /home/ashgatsy/test/logs/pcb-example1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (train_data, test_data), info = tfds.load(\n",
    "#     \"imdb_reviews/subwords8k\",\n",
    "#     split=(tfds.Split.TRAIN, tfds.Split.TEST),\n",
    "#     with_info=True,\n",
    "#     as_supervised=True,\n",
    "# )\n",
    "# encoder = info.features[\"text\"].encoder\n",
    "\n",
    "# # Shuffle and pad the data.\n",
    "# train_batches = train_data.shuffle(1000).padded_batch(\n",
    "#     10, padded_shapes=((None,), ())\n",
    "# )\n",
    "# test_batches = test_data.shuffle(1000).padded_batch(\n",
    "#     10, padded_shapes=((None,), ())\n",
    "# )\n",
    "# train_batch, train_labels = next(iter(train_batches))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an embedding layer.\n",
    "# embedding_dim = 16\n",
    "# embedding = tf.keras.layers.Embedding(encoder.vocab_size, embedding_dim)\n",
    "# # Configure the embedding layer as part of a keras model.\n",
    "# model = tf.keras.Sequential(\n",
    "#     [\n",
    "#         embedding, # The embedding layer should be the first layer in a model.\n",
    "#         tf.keras.layers.GlobalAveragePooling1D(),\n",
    "#         tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "#         tf.keras.layers.Dense(1),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # Compile model.\n",
    "# model.compile(\n",
    "#     optimizer=\"adam\",\n",
    "#     loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "#     metrics=[\"accuracy\"],\n",
    "# )\n",
    "\n",
    "# # Train model for one epoch.\n",
    "# history = model.fit(\n",
    "#     train_batches, epochs=1, validation_data=test_batches, validation_steps=20\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set up a logs directory, so Tensorboard knows where to look for files.\n",
    "# log_dir='/home/ashgatsy/test/logs/imdb-example/'\n",
    "\n",
    "# # Save Labels separately on a line-by-line manner.\n",
    "# with open(os.path.join(log_dir, 'metadata.tsv'), \"w\") as f:\n",
    "#   for subwords in encoder.subwords:\n",
    "#     f.write(\"{}\\n\".format(subwords))\n",
    "#   # Fill in the rest of the labels with \"unknown\".\n",
    "#   for unknown in range(1, encoder.vocab_size - len(encoder.subwords)):\n",
    "#     f.write(\"unknown #{}\\n\".format(unknown))\n",
    "\n",
    "\n",
    "# # Save the weights we want to analyze as a variable. Note that the first\n",
    "# # value represents any unknown word, which is not in the metadata, here\n",
    "# # we will remove this value.\n",
    "# weights = tf.Variable(model.layers[0].get_weights()[0][1:])\n",
    "# # Create a checkpoint from embedding, the filename and key are the\n",
    "# # name of the tensor.\n",
    "# checkpoint = tf.train.Checkpoint(embedding=weights)\n",
    "# checkpoint.save(os.path.join(log_dir, \"embedding.ckpt\"))\n",
    "\n",
    "# # Set up config.\n",
    "# config = projector.ProjectorConfig()\n",
    "# embedding = config.embeddings.add()\n",
    "# # The name of the tensor will be suffixed by `/.ATTRIBUTES/VARIABLE_VALUE`.\n",
    "# embedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
    "# embedding.metadata_path = 'metadata.tsv'\n",
    "# projector.visualize_embeddings(log_dir, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Now run tensorboard against on log data we just saved.\n",
    "# !tensorboard --logdir /home/ashgatsy/test/logs/imdb-example/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
