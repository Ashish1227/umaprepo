{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Embeddings Extraction Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook we can generate embeddings using verious pre-trained models and upload them to qdrant, we also make use of tensorboard\n",
    "# to visualize the embeddings, the directory where the tensorboard related information such as checkpoints/logs etc will be stored is\n",
    "# hardcoded as of now, user needs to change it according to their preference. UMAP visualization of the extracted embeddings is achieved\n",
    "# in this notebook. Dataset used is DeepPCB dataset which has 3001 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "import umap.umap_ as umap\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.patches as mpatches\n",
    "from tensorboard.plugins import projector # type: ignore\n",
    "from sklearn.metrics import silhouette_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loading and temp+test images extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/home/ashgatsy/DeepPCB-master/PCBData\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = []\n",
    "defect = []\n",
    "defectlog = []\n",
    "path_2 = [os.path.join(PATH,dir) for dir in os.listdir(PATH) if '.' not in dir]\n",
    "for p in tqdm(path_2,total=len(path_2)):\n",
    "    path_3 = os.path.join(p,sorted(os.listdir(p))[0])\n",
    "    normal +=[os.path.join(path_3,dir)for dir in os.listdir(path_3) if 'temp' in dir]\n",
    "    defect +=[os.path.join(path_3,dir)for dir in os.listdir(path_3) if 'test' in dir]\n",
    "    path_4 = os.path.join(p,sorted(os.listdir(p))[1])\n",
    "    defectlog +=[os.path.join(path_4,dir)for dir in os.listdir(path_4)]\n",
    "normal.sort()\n",
    "defect.sort()\n",
    "defectlog.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img0 = []\n",
    "img1 = []\n",
    "for img_path in tqdm(normal,total=len(normal)):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.resize(img,(128,128))\n",
    "    img0.append(img)\n",
    "for img_path in tqdm(defect,total=len(defect)):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.resize(img,(128,128))\n",
    "    img1.append(img)\n",
    "img0 = np.array(img0)\n",
    "img1 = np.array(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_to_rgb(bnw_images):\n",
    "    # function that converts numpy array of bnw images to numpy array of rgb images\n",
    "    num_images, height, width = bnw_images.shape\n",
    "    rgb_images = np.zeros((num_images, height, width, 3), dtype=np.uint8)\n",
    "    # Set all three channels to the same intensity (grayscale value)\n",
    "    for i in range(num_images):\n",
    "        rgb_images[i, :, :, 0] = bnw_images[i]  # Red channel\n",
    "        rgb_images[i, :, :, 1] = bnw_images[i]  # Green channel\n",
    "        rgb_images[i, :, :, 2] = bnw_images[i]  # Blue channel\n",
    "\n",
    "    rgb_images = rgb_images.transpose(0, 3, 1, 2)\n",
    "    return rgb_images\n",
    "\n",
    "rgb_images0 = conv_to_rgb(img0)\n",
    "rgb_images1 = conv_to_rgb(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images, height, width = img0.shape\n",
    "rgb_images = np.zeros((num_images, height, width, 3), dtype=np.uint8)\n",
    "\n",
    "# Set all three channels to the same intensity (grayscale value)\n",
    "for i in range(num_images):\n",
    "    rgb_images[i, :, :, 0] = img0[i]  # Red channel\n",
    "    rgb_images[i, :, :, 1] = img0[i]  # Green channel\n",
    "    rgb_images[i, :, :, 2] = img0[i]  # Blue channel\n",
    "\n",
    "rgb_images = rgb_images.transpose(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import any pre-trained Model and preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load('facebookresearch/WSL-Images', 'resnext101_32x8d_wsl')\n",
    "\n",
    "def slice_model(original_model, from_layer=None, to_layer=None):\n",
    "    return nn.Sequential(*list(original_model.children())[from_layer:to_layer])\n",
    "\n",
    "model_conv_features = slice_model(model, to_layer=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define required transformations according the model imported\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    # transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transforms(rgb_images,transform):\n",
    "    # function to apply required transformation on the dataset images\n",
    "    transformed_images0 = []\n",
    "    rgb_images = rgb_images.transpose(0, 2, 3, 1)\n",
    "    for img_np in rgb_images:\n",
    "        # Convert numpy array to PIL Image\n",
    "        # print(img_np.shape)\n",
    "        img_pil = Image.fromarray(img_np)\n",
    "\n",
    "        # Apply the transformation\n",
    "        img_transformed = transform(img_pil)\n",
    "\n",
    "        # Convert the transformed image back to numpy array\n",
    "        img_transformed_np = np.array(img_transformed)\n",
    "\n",
    "        # Append to the list\n",
    "        transformed_images0.append(img_transformed_np)\n",
    "    return transformed_images0\n",
    "\n",
    "transformed_images0 = apply_transforms(rgb_images0,preprocess)\n",
    "transformed_images1 = apply_transforms(rgb_images1,preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, targets=None, transform=None):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        if self.targets is not None:\n",
    "            return sample, self.targets[idx]\n",
    "        else:\n",
    "            return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset0 = MyDataset(data=transformed_images0)\n",
    "my_dataset1 = MyDataset(data=transformed_images1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_count = multiprocessing.cpu_count()\n",
    "batch_size = 8\n",
    "my_dataloader0 = DataLoader(dataset=my_dataset0, batch_size=batch_size, shuffle=True, num_workers=cpu_count)\n",
    "my_dataloader1 = DataLoader(dataset=my_dataset1, batch_size=batch_size, shuffle=True, num_workers=cpu_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(dataloader,model_conv_features):\n",
    "    features_list=[]\n",
    "    # this function returns the populated features list\n",
    "    model_conv_features\n",
    "    for batch in tqdm(dataloader):\n",
    "        # print(batch.shape)\n",
    "        image_batch = batch\n",
    "        # image_batch = torch.stack(image_batch) \n",
    "\n",
    "        with torch.no_grad():\n",
    "            features_batch = model_conv_features(image_batch).flatten(start_dim=1)\n",
    "        features_list.append(features_batch)\n",
    "\n",
    "    return features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list0 = get_features(my_dataloader0,model_conv_features)\n",
    "features_list1 = get_features(my_dataloader1,model_conv_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload/Retrieve the embeddings : qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "qdrant = QdrantClient(url='http://132.186.158.40:6333/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_embeddings(features_list):\n",
    "  # Reshape each tensor to remove the batch dimension\n",
    "  embeddings = [tf.reshape(tensor, [-1, 2048]) for tensor in features_list]\n",
    "\n",
    "  # Concatenate the reshaped tensors\n",
    "  concatenated_embeddings = tf.concat(embeddings, axis=0)\n",
    "\n",
    "  return concatenated_embeddings.numpy().tolist()\n",
    "\n",
    "individual_embeddings0 = concatenate_embeddings(features_list0)\n",
    "individual_embeddings1 = concatenate_embeddings(features_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qdarnt_upload(collection_name,csize,vectors):\n",
    "    qdrant.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(size=2048,distance=Distance.COSINE),\n",
    "    )\n",
    "    for i, embedding in enumerate(vectors):\n",
    "        qdrant.upsert(collection_name=collection_name, points=[{\n",
    "            'id': i,  # Unique ID for each data point\n",
    "            'vector': embedding,  # Convert numpy array to list\n",
    "            'payload': {'text': f'Embedding {i}'}  # Optional payload (metadata)\n",
    "        }])\n",
    "    print(\"Embeddings inserted successfully!\")\n",
    "\n",
    "qdarnt_upload(\"check1\",1501,individual_embeddings0)\n",
    "qdarnt_upload(\"check2\",1500,individual_embeddings1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_points(collection_name,client,num_vectors):\n",
    "    res = client.scroll(\n",
    "    collection_name=collection_name,\n",
    "    scroll_filter=None,\n",
    "    limit=num_vectors,\n",
    "    with_payload=True,\n",
    "    with_vectors=True,\n",
    "    )\n",
    "    return res\n",
    "\n",
    "ret_points0 = retrieve_points(\"check1\",qdrant,1501)\n",
    "ret_points1 = retrieve_points(\"check2\",qdrant,1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(points):\n",
    "    ret_embeddings = []\n",
    "    ret_ids = [] #no need to keep anything here since id is just the index of the retrieved vector. (int(ret_points0[0][0].payload['text'].split()[1]))\n",
    "    for record in points[0]:\n",
    "        ret_embeddings.append(record.vector)\n",
    "\n",
    "    return ret_embeddings\n",
    "\n",
    "ret_embeddings0 = extract_embeddings(ret_points0)\n",
    "ret_embeddings1 = extract_embeddings(ret_points1)\n",
    "tot_embeddings = ret_embeddings0+ret_embeddings1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard Projection to visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_embeddings(embeddings, num_clusters):\n",
    "    # Flatten the list of tensors into a single 2D array\n",
    "    flattened_embeddings = np.array(embeddings)\n",
    "\n",
    "    # Perform K-Means clustering\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=0)  # Set random_state for reproducibility\n",
    "    kmeans.fit(flattened_embeddings)\n",
    "\n",
    "    # Get cluster labels for each data point\n",
    "    cluster_labels = kmeans.labels_\n",
    "\n",
    "    return cluster_labels\n",
    "\n",
    "cluster_labels = cluster_embeddings(tot_embeddings,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir='/home/ashgatsy/test/logs/pcb-example1/'\n",
    "\n",
    "def tensorboard_umap(umap_embeddings,cluster_labels):\n",
    "    embedding_var = tf.Variable(umap_embeddings,name='embedding')\n",
    "    checkpoint = tf.train.Checkpoint(embedding=embedding_var)\n",
    "    checkpoint.save(os.path.join(log_dir,'ts_embedding.ckpt'))\n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding = config.embeddings.add()\n",
    "    embedding.tensor_name = 'embedding/.ATTRIBUTES/VARIABLE_VALUE'  # Assuming saved tensor\n",
    "\n",
    "    # Create metadata.tsv (optional): Each line: index<tab>cluster_label\n",
    "    with open('/home/ashgatsy/test/logs/pcb-example1/metadata.tsv', 'w') as f:\n",
    "        column_names = [\"Id\", \"Cluster_Label\"]\n",
    "        f.write('\\t'.join(column_names) + '\\n')\n",
    "        for i, label in enumerate(cluster_labels):\n",
    "            f.write(f\"{i}\\t{label}\\n\")\n",
    "\n",
    "    embedding.metadata_path = 'metadata.tsv'\n",
    "\n",
    "    projector.visualize_embeddings(log_dir, config)\n",
    "\n",
    "tensorboard_umap(tot_embeddings,cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir /home/ashgatsy/test/logs/pcb-example1/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
